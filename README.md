# Coursera Machine Learning Exercises
This repository contains exercises from the renowned Machine Learning course led by Andrew Ng. The exercises, implemented in MATLAB, cover foundational concepts of machine learning, including:

## Exercise 1: Linear Regression
This exercise focuses on linear regression with multiple variables. It includes:

- Warm-up Exercise: Basic MATLAB function.
- Computing Cost (for One Variable): Implementing cost function for linear regression.
- Gradient Descent (for One Variable): Applying gradient descent to find the minimum cost.
- Feature Normalization: Normalizing features for better model performance.
- Computing Cost (for Multiple Variables): Extending the cost function for multiple variables.
- Gradient Descent (for Multiple Variables): Implementing gradient descent in a multi-variable context.
- Normal Equations: Using normal equations to solve linear regression.

## Exercise 2: Logistic Regression
This exercise delves into logistic regression, covering:

- Sigmoid Function: Implementing the sigmoid function.
- Logistic Regression Cost: Calculating the cost function for logistic regression.
- Logistic Regression Gradient: Computing the gradient of the cost function.
- Predict: Creating a prediction function based on logistic regression.
- Regularized Logistic Regression Cost: Implementing cost function with regularization.
- Regularized Logistic Regression Gradient: Computing the gradient for the regularized cost.

## Exercise 3: Multi-class Classification and Neural Networks
This exercise explores multi-class classification and neural networks:

- Regularized Logistic Regression: Implementing logistic regression with regularization.
- One-vs-All Classifier Training: Training a classifier to distinguish between more than two classes.
- One-vs-All Classifier Prediction: Making predictions using the trained classifier.
- Neural Network Prediction Function: Implementing a function to make predictions using a neural network.
- Each exercise includes various MATLAB files where the necessary functions are defined, providing hands-on experience with these fundamental machine learning techniques.

## Exercise 4: Neural Network Learning
Focuses on implementing neural networks:

- Feedforward and Cost Function: Computing the cost for a two-layer neural network.
- Regularized Cost Function: Enhancing the cost function with regularization.
- Sigmoid Gradient: Implementing the gradient of the sigmoid function.
- Neural Network Gradient (Backpropagation): Applying backpropagation to compute gradients.
- Regularized Gradient: Extending gradient calculation with regularization.

## Exercise 5: Regularized Linear Regression and Bias/Variance
Deals with understanding model complexity:

- Regularized Linear Regression Cost Function: Implementing cost function with regularization.
- Regularized Linear Regression Gradient: Calculating gradients for regularized regression.
- Learning Curve: Analyzing learning curves to understand bias and variance.
- Polynomial Feature Mapping: Creating polynomial features for regression.
- Validation Curve: Using validation curves to select regularization parameters.

## Exercise 6: Support Vector Machines
Explores SVM for classification tasks:

- Gaussian Kernel: Implementing the Gaussian kernel for SVMs.
- Parameters for Dataset 3: Selecting optimal parameters (C, sigma) for SVMs.
- Email Preprocessing: Processing emails for feature extraction.
- Email Feature Extraction: Extracting features from preprocessed emails.

## Exercise 7: K-Means Clustering and PCA
Covers unsupervised learning techniques:

- Find Closest Centroids (k-Means): Identifying closest centroids in k-Means.
- Compute Centroid Means (k-Means): Computing means of centroids in k-Means.
- PCA: Implementing Principal Component Analysis.
- Project Data (PCA): Projecting data into a lower-dimensional space.
- Recover Data (PCA): Recovering original data from the compressed representation.

## Exercise 8: Anomaly Detection and Recommender Systems
Focuses on advanced applications of ML:

- Estimate Gaussian Parameters: Estimating parameters for Gaussian models in anomaly detection.
- Select Threshold: Selecting threshold for identifying anomalies.
- Collaborative Filtering Cost: Implementing cost function for collaborative filtering.
- Collaborative Filtering Gradient: Computing gradient for collaborative filtering.
- Regularized Cost & Gradient: Enhancing collaborative filtering with regularization.
- Each exercise includes detailed MATLAB implementations, offering practical insight into these advanced machine learning topics.